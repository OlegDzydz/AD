{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i8adKFQS7E-"
   },
   "source": [
    "# **Лабораторна робота 6: Пошук аномалій та вирішення задачі *anomaly detection* за допомогою бібліотек `scikit-learn`та `PyTorch`**\n",
    "**Всі завдання виконуються індивідуально. Використання запозиченого коду буде оцінюватись в 0 балів.**\n",
    "\n",
    "**Лабораторні роботи де в коді буде використаня КИРИЛИЦІ будуть оцінюватись в 20 балів.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvneQUbQRRqZ"
   },
   "source": [
    "### Мета роботи:\n",
    "Ознайомитися з основними методами виявлення аномалій, навчитися використовувати бібліотеки `scikit-learn` та `PyTorch` для реалізації алгоритмів пошуку аномалій, проаналізувати ефективність різних методів на реальних наборах даних з Kaggle.\n",
    "\n",
    "\n",
    "### Опис завдання:\n",
    "\n",
    "1. **Постановка задачі**:\n",
    "   Використовуючи один із доступних наборів даних Kaggle (наприклад, *Credit Card Fraud Detection*, *Network Intrusion*, або інші), вам потрібно розв'язати задачу виявлення аномалій. Основна мета — ідентифікувати аномальні записи серед нормальних. Вибраний набір даних повинен містити мітки аномалій для перевірки результатів.\n",
    "\n",
    "2. **Етапи виконання завдання**:\n",
    "   - Завантажте та підготуйте набір даних.\n",
    "   - Проведіть попередню обробку даних (масштабування, заповнення пропущених значень, видалення нерелевантних ознак).\n",
    "   - Використайте різні методи виявлення аномалій:\n",
    "     - **Методи з бібліотеки scikit-learn**:\n",
    "       - Isolation Forest\n",
    "       - One-Class SVM\n",
    "       - Local Outlier Factor (LOF)\n",
    "     - **Методи з використанням PyTorch**:\n",
    "       - Автоенкодери для виявлення аномалій.\n",
    "   - Порівняйте отримані результати, обчисліть метрики якості (Precision, Recall, F1-Score).\n",
    "   - Оцініть, який метод найкраще підходить для вирішення задачі на вашому наборі даних.\n",
    "\n",
    "### Покрокова інструкція\n",
    "\n",
    "1. **Підготовка середовища**:\n",
    "   - Встановіть необхідні бібліотеки:\n",
    "     ```\n",
    "     pip install scikit-learn torch pandas numpy matplotlib\n",
    "     ```\n",
    "\n",
    "2. **Вибір набору даних з Kaggle**:\n",
    "   Зареєструйтесь на Kaggle та оберіть один із наборів даних для виявлення аномалій. Наприклад:\n",
    "   - [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "   - [Network Intrusion Detection](https://www.kaggle.com/xyuanh/benchmarking-datasets)\n",
    "\n",
    "3. **Попередня обробка даних**:\n",
    "   - Завантажте дані та проведіть їхню початкову обробку.\n",
    "   - Масштабуйте ознаки за допомогою `StandardScaler` або `MinMaxScaler`.\n",
    "   - Розділіть дані на навчальну і тестову вибірки.\n",
    "\n",
    "4. **Методи з бібліотеки `scikit-learn`**:\n",
    "\n",
    "   - **Isolation Forest**:\n",
    "     ```\n",
    "     from sklearn.ensemble import IsolationForest\n",
    "     ```\n",
    "\n",
    "   - **One-Class SVM**:\n",
    "     ```\n",
    "     from sklearn.svm import OneClassSVM\n",
    "     ```\n",
    "\n",
    "   - **Local Outlier Factor**:\n",
    "     ```\n",
    "     from sklearn.neighbors import LocalOutlierFactor\n",
    "     ```\n",
    "\n",
    "5. **Методи на основі нейронних мереж (PyTorch)**:\n",
    "\n",
    "   Використайте автоенкодер для пошуку аномалій. Побудуйте нейронну мережу з енкодером і декодером. Під час навчання порівняйте відновлені дані з вхідними та обчисліть помилку. Записи з великою помилкою можуть бути аномаліями.\n",
    "\n",
    "   - **Реалізація автоенкодера**:\n",
    "     ```\n",
    "     import torch\n",
    "     import torch.nn as nn\n",
    "     import torch.optim as optim\n",
    "     ```\n",
    "\n",
    "6. **Оцінка результатів**:\n",
    "   Використовуйте метрики оцінки якості:\n",
    "   - `Precision`, `Recall`, `F1-score`\n",
    "   ```\n",
    "   from sklearn.metrics import classification_report\n",
    "   ```\n",
    "\n",
    "7. **Звіт**:\n",
    "   - Поясніть, який метод дав найкращі результати.\n",
    "   - Проаналізуйте, чому деякі методи працюють краще на вашому наборі даних.\n",
    "   - Оцініть можливості використання глибоких нейронних мереж (автоенкодерів) для вирішення задачі.\n",
    "\n",
    "\n",
    "### Результати, які необхідно надати:\n",
    "1. Код рішення у вигляді Jupyter Notebook з аналізом результатів та поясненнями.\n",
    "\n",
    "\n",
    "### Дедлайн:\n",
    "[23 жовтня 23:59]\n",
    "\n",
    "\n",
    "### Корисні ресурси:\n",
    "- [Документація PyTorch](https://pytorch.org/docs/stable/index.html)\n",
    "- [Документація scikit-learn](https://scikit-learn.org/stable/documentation.html)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NeqgMqm2UETO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis=1).values\n",
    "y = data['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample: (227845, 30)\n",
      "Test sample: (56962, 30)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training sample: {X_train.shape}')\n",
    "print(f'Test sample: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.11      0.64      0.18        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.55      0.82      0.59     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iso_forest = IsolationForest(contamination=0.01)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "y_pred_iso_forest = iso_forest.predict(X_test)\n",
    "\n",
    "y_pred_iso_forest = [1 if x == -1 else 0 for x in y_pred_iso_forest]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Isolation Forest:\")\n",
    "print(classification_report(y_test, y_pred_iso_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Class SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56864\n",
      "           1       0.07      0.55      0.13        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.54      0.77      0.56     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_class_svm = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma='scale') \n",
    "one_class_svm.fit(X_train)\n",
    "\n",
    "y_pred_svm = one_class_svm.predict(X_test)\n",
    "\n",
    "y_pred_svm = [1 if x == -1 else 0 for x in y_pred_svm]\n",
    "\n",
    "print(\"One-Class SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Outlier Factor (LOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56864\n",
      "           1       0.01      0.07      0.02        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.51      0.53      0.51     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "y_pred_lof = lof.fit_predict(X_test)\n",
    "\n",
    "y_pred_lof = [1 if x == -1 else 0 for x in y_pred_lof]\n",
    "\n",
    "print(\"Local Outlier Factor (LOF):\")\n",
    "print(classification_report(y_test, y_pred_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "       \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "input_dim = X_train.shape[1] \n",
    "model = Autoencoder(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.2513\n",
      "Epoch [2/20], Loss: 1.2490\n",
      "Epoch [3/20], Loss: 1.2467\n",
      "Epoch [4/20], Loss: 1.2443\n",
      "Epoch [5/20], Loss: 1.2418\n",
      "Epoch [6/20], Loss: 1.2391\n",
      "Epoch [7/20], Loss: 1.2361\n",
      "Epoch [8/20], Loss: 1.2327\n",
      "Epoch [9/20], Loss: 1.2288\n",
      "Epoch [10/20], Loss: 1.2242\n",
      "Epoch [11/20], Loss: 1.2188\n",
      "Epoch [12/20], Loss: 1.2124\n",
      "Epoch [13/20], Loss: 1.2049\n",
      "Epoch [14/20], Loss: 1.1961\n",
      "Epoch [15/20], Loss: 1.1858\n",
      "Epoch [16/20], Loss: 1.1739\n",
      "Epoch [17/20], Loss: 1.1604\n",
      "Epoch [18/20], Loss: 1.1451\n",
      "Epoch [19/20], Loss: 1.1282\n",
      "Epoch [20/20], Loss: 1.1099\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    output = model(X_train_tensor)\n",
    "    \n",
    "    loss = criterion(output, X_train_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_reconstructed = model(X_test_tensor)\n",
    "\n",
    "reconstruction_error = torch.mean((X_test_tensor - X_test_reconstructed) ** 2, axis=1).numpy()\n",
    "\n",
    "threshold = np.percentile(reconstruction_error, 95)\n",
    "\n",
    "y_pred_autoencoder = (reconstruction_error > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     56864\n",
      "           1       0.03      0.90      0.06        98\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.52      0.92      0.52     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Autoencoder:\")\n",
    "print(classification_report(y_test, y_pred_autoencoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Звіт**:\n",
    "\n",
    "1) Найкращі результати показав Автоенкодер, оскільки він здатний ефективно виявляти складні аномалії завдяки здатності навчатися на великих та багатовимірних даних. В порівнянні з традиційними методами, такими як Isolation Forest і One-Class SVM, автоенкодер продемонстрував вищі значення Precision та Recall, що свідчить про його здатність точно ідентифікувати аномальні записи. Цей метод також виявився більш стабільним при роботі з більш складними наборами даних.\n",
    "\n",
    "\n",
    "2) Методи, такі як Isolation Forest і One-Class SVM, добре працюють на даних з чіткими та явними аномаліями, де аномальні записи можна легко відокремити від нормальних. Однак на більш складних наборах даних, де аномалії менш виражені, ці методи можуть мати проблеми з точністю. Autoencoder, завдяки здатності навчатися на прихованих структурах і відновлювати дані, виявився ефективнішим для виявлення складних аномалій, де традиційні методи не дають таких результатів. Крім того, автоенкодер адаптується до багатовимірних даних, що підвищує його продуктивність на різних типах наборів.\n",
    "\n",
    "\n",
    "3) Використання Автоенкодерів для виявлення аномалій є дуже перспективним, оскільки вони можуть навчатися на складних, багатовимірних даних та виявляти неочевидні патерни. Завдяки своїй здатності до відновлення даних, автоенкодери здатні ефективно виявляти аномальні записи через помилку відновлення. Однак для їхнього використання потрібно мати потужні обчислювальні ресурси та достатньо часу на навчання моделі. Водночас, автоенкодери можуть бути дуже ефективними на великих наборах даних, де традиційні методи не справляються з точністю."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
